[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Grace M. Horne",
    "section": "",
    "text": "Personal statement\n        I am interested in herbivore-plant interactions, especially between Lepidopterans and their food-plants. I seek to qualify how these relationships will change in response to disturbances such as exotic species invasion and climate change. I am eager to engage with community scientists in data collection and environmental education.\n      \n    \n  \n\n\n\n\n\n\n\nCurriculum vitae\n\n\nEducation\n\n\n2021 – present\nUniversity of California, Davis\nPh.D. Entomology, in progress\nAdvisor: Dr. Emily Meineke\n2017 – 2021\nColby College, Waterville, ME\nB.A. Biology (honors) & Environmental Science\nMagna cum laude with distinction in both majors\nThesis: “Reduced performance of ash-specialist caterpillars on non-native, cultivated Oleaceous plants”\nSpring 2020\nRound River Conservation Studies, Maun, Botswana\n\nRound River Conservation Studies (RRCS) is an environmental organization operating at the nexus of conservation and education. We explored the complex relationship between conservation, people, and wildlife, mostly in Mababe, Botswana. We worked with local experts to design and maintain wildlife monitoring systems to be used to substantiate economic and environmental decisions. This program helped me to develop fluency in field work, responses to human-wildlife conflict, and communication with local stakeholders.\n\n\nTeaching\n\n\nFall 2022\nUC Davis Department of Environmental Science & Policy, Davis, CA\nGeneral Ecology Teaching Assistant\n\nSpring 2022\nUC Davis Department of Evolution & Ecology, Davis, CA\nIntroduction to Ecology Teaching Assistant\n\nWinter 2022, 2023, 2024\nUC Davis Department of Entomology & Nematology, Davis, CA\nNatural History of Insects Guest Lecturer\nLecture: “Insects & Climate Change”\n\nFall 2018\nColby College Mathematics Department, Waterville, ME\nSeries and Multi-variable Calculus Teaching Assistant\n\n\nEmployment & Commissions\n\n\n2022 – present\nPicnic Day Co-Chair\nUniversity of California, Davis\n\n2022 – present\nEntomology Seminar Series Committee Member\nUniversity of California, Davis\n\n2018 – 2021\nEducation Staff & Undergraduate Researcher\nThe Caterpillar Lab, Marlborough, NH\n\nThe Caterpillar Lab (TCL) is an environmental education organization focused on inviting people—kids and adults alike—to share in stories of ecology, evolution, and natural history. I have presented at over 30 locations including elementary school classrooms, botanic gardens, and children’s museums, |with a goal to ignite in us a curiosity of the world we live in, from the smallest leaf-miners to the tallest trees.\n\n2019 – 2020\nStudent Assistant\nColby College Olin Science Library, Waterville, ME\n\n\nFellowships & Grants\n\n\nExternal\n2023\nNSF Graduate Research Fellowship ($147,000)\n\n2022\nCNPS Student Research Grant Award ($595)\n\nInternal\n2023\nJastro-Shields Graduate Student Research Award ($2,650)\n\n2022\nMcBeth Memorial Scholarship\nHenry A. Jastro Research Scholarship (combined, $2,650)\n\n2020\nColby College Students’ Special Project Grant ($500)\n\n2017 – 2021\nColby College National Merit Scholarship ($8,000)\n\n\nHonors & Awards\n\n\n2021\nColby College Off-Campus Study Photo Contest (1st place)\nEntry title, description: “Cricket Cricket,” a macro-photograph of a setotojane katydid\n\n2017 – 2021\nColby College National Merit Scholar\nDean’s Honors (in every qualifying semester)\n\n\nPublications\n\n\nFull text publication access link (view “attachments”).\n\n\n\n\n\n\n\n1.     Horne, Grace M., Rea Manderino, and Samuel P. Jaffe. (2023) Specialist herbivore performance on introduced plants during native host decline. Environmental Entomology.\n\n\n\n\n\n\n\nPresentations\n\n\n7.     Horne, Grace M.*, John de Benedictis, Matthew L. Forister, and Emily K. Meineke. (2023) Long-term datasets reveal climatic nuance in moth responses to global change. Biology of Butterflies Conference.\n\n6.     Horne, Grace M.* (2023) Climate change and our backyard fauna: a tale of the moths of Davis, CA. Atria Covell Gardens.\n\n5.     Meineke, Emily K., Alison Colwell*, Grace M. Horne, and David Eng. (2022) Harnessing the Power of Inadvertent Collecting: Bias assessments in support of use of herbarium specimens for entomology research. California Native Plant Society Conference.\n\n4.     Horne, Grace M.*, John de Benedictis, Matthew L. Forister, and Emily K. Meineke. (2022) Two new datasets reveal long-term moth community shifts in a biodiversity hotspot. Ecological Society of American Annual Meeting.\n\n3.     Horne, Grace M.* (2021) Reduced performance of ash-specialist caterpillars on non-native, cultivated Oleaceous plants. Colby Liberal Arts Symposium.\n\n2.     Horne, Grace M.* (2020) Will non-native Oleaceae family plants preserve and support native ash-feeding Sphingid populations in post-emerald ash borer forests? Coastal Maine Botanical Gardens.\n\n1.     Horne, Grace M.* (2019) Canid infectious disease transmission via domestic dogs to Lycaon pictus. Colby Liberal Arts Symposium.\n\n* presenting author\n\n\nWorkshops\n\n\n2023\nUsing GitHub with R(Studio), Williams Lab @ UCD, February 17\n\n2022\nUsing GitHub with R(Studio), Meineke Lab @ UCD, October 28\n\n\nOutreach & Extension\n\n\nBold type indicates an institution serving a majority low-income population\n\n2024\nBiodiversity Museum Day, February 10\n\n2023\nBohart Museum Open House: “Beetles!,” January 22\nMuseum Biodiversity Day, February 18\nUC Davis Picnic Day: Entomology Exhibit, April 15†\nBohart Museum @ Sacramento Earth Day, April 23\nCity Nature Challenge BioBlitz @ Davis Arboretum, April 29†\nCity Nature Challenge @ SMUD Museum of Science, April 29\nGo Wild Yolo Library @ Capay Open Space Park, June 10\n\n2022\nSTEM Squad Event @ Winters Middle School, February 24\nSTEM Squad Event @ Winters Middle School, April 14\nUC Davis Picnic Day: Entomology Exhibit, April 23†\nDavis, CA Loopalooza: Bohart Museum Station, May 1\nSTEM Squad X Google Event @ Holy Rosary School, May 13\nBohart Museum Open House: “Bugs in Ag,” May 28\nBohart Museum Open House: “Insects, Art, & Culture,” October 15\nGo Wild Yolo Library @ Capay Open Space Park, October 22\n\n2021\nDiscovery Museum Residency, June 7 - 21\nTroy, NH Girl Scouts of America Program, June 28\nBerkshire Botanical Garden Residency, July 3 - 4\nRoyalston, MA Library Program, July 15\nBedrock Gardens Program, July 18\nSquam Lakes Natural Science Center Program, July 28 - August 3\nVermont Institute of Natural Science Program, August 7\n\n2020*\nCoastal Maine Botanical Gardens Residency, August 29\n\n2019\nLincoln-Eliot School Program, May 29\nHorace Mann School Program, May 30\nRandall Library Program, May 31\nKeene Farmer’s Market Booth, June 1\nFramingham Public Library Program, June 2\nNH Audubon Pollinator Party, June 15\nBrattleboro Farmer’s Market Booth, June 22\nWildwood Camp Program, June 25\nAldworth Manor Program, June 27\nNorth Branch Nature Center Program, June 30\nBradford Conservation Commission Program, July 9\nSquam Lakes Natural Science Center Program, July 11\nWarren Public Library Program, July 13\nBrattleboro, VT Museum Program, July 29\nCoastal Maine Botanical Gardens Residency, July 22 - 27\nFranklin Pierce University Camp Program, July 30\nNew England Botanic Garden at Tower Hill Program, August 3\n\n2018\nNorth Branch Nature Center BioBlitz, July 21 – 22\nArnold Arboretum Program, July 28\nBoston Museum of Science Exhibit, July 11\nNew England Botanic Garden at Tower Hill Program, July 25 – 26\nDiscovery Museum Residency, July 28 – August 3\n\n* Senior thesis data collection, COVID-19 pandemic\n† Independently designed and presented\n\n\nProfessional Affiliations\n\n\n2022 – present\nEcological Society of America\nEntomological Society of America\n\n\nReviews\n\n\nProceedings of the Royal Society B\nScientific Reports\n\n\nRelevant Coursework\n\n\nCell Biology\nConservation Biology\nEnvironmental & Natural Resource Economics\nGIS & Spatial Analysis\nIntroduction to R\nSeries & Multivariable Calculus\nTaxonomy of Flowering Plants\nWoody Plants\nProblems in Environmental Science\nEvolutionary Analysis\nCommunity Ecology\nForest Ecosystems\nIntroduction to Entomology\nInsect Physiology\nScientific Illustration in Illustrator & Photoshop\nTrait-Based Ecology\nInsect Ecology\nBayesian Statistics"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Grace M. Horne",
    "section": "",
    "text": "Specialist herbivore performance on introduced plants during native host decline\nMy senior undergraduate thesis was published in Environmental Entomology in January 2023. It is free to view at this link: https://doi.org/10.1093/ee/nvac107.\nDuring the socially-distant summer of 2020, I raised hundreds of caterpillars with the goal of helping to answer a single question: what will become of ash-eating insects once ash disappears (or nearly disappears) from New England and northeastern forests? To chip away at this mammoth question—all insects are sure to respond differently!—I reared three species of hawkmoth caterpillars on four different host plants in the same plant family as their primary host plant, ash.\nOver the course of the experiment, I began to notice differences in how the three species tolerated each host plant: Sphinx kalmiae did quite well on lilac, but Sphinx chersis perished entirely on the same plant. These are the differences that colored my narrative. The problem of disappearing ash will not be discovered by painting in broad strokes.\n\n\n\n\n\n\nThe impact of a plant species becoming locally rare can have rippling impacts up trophic levels."
  },
  {
    "objectID": "outreach.html",
    "href": "outreach.html",
    "title": "Grace M. Horne",
    "section": "",
    "text": "Outreach\n\nBohart Museum of Entomology\nCurrently, at UC Davis, I volunteer with the Bohart Museum of Entomology whenever I get the chance. Regularly throughout the year, the Museum puts on open houses. Each open house has a theme, from ‘Beetles’ to ‘Insects in Agriculture,’ with experts and enthusiasts spreading their knowledge and answering questions. There is a focus on early education with crafts and tactile exhibits targeted towards little hands. These open houses are family friendly and open to the public. Visit their website for more details: https://bohart.ucdavis.edu.\n\n\nThe Caterpillar Lab\nDuring college, I spent my summers interning at The Caterpillar Lab, an environmental outreach organization in my hometown of Marlborough, NH. With the Lab, I was able to travel across New England to libraries, classrooms, botanic gardens, farmers markets, museums of science, etc. We brought our mobile exhibits of caterpillars displaying defensive coloration, neat host plant associations, and interesting tales of population decline everywhere we went. A semi-comprehensive list of all of the programs I attended with The Caterpillar Lab is available in the outreach section of my CV. Visit their website for more details: https://www.thecaterpillarlab.org."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Grace M. Horne",
    "section": "",
    "text": "Resources\nBelow is a working list of the resources I use for my research. You can view my current public projects on my GitHub profile, @gracehorne.\n\nR\n\nR for Data Science\nStack Overflow\n(R) Coding Club\nHappy Git with R"
  },
  {
    "objectID": "index.html#personal-statement",
    "href": "index.html#personal-statement",
    "title": "Grace M. Horne",
    "section": "Personal Statement",
    "text": "Personal Statement\nI am interested in herbivore-plant interactions, especially between Lepidopterans and their food-plants. I seek to qualify how these relationships will change in response to disturbances such as exotic species invasion and climate change. I am eager to engage with community scientists in data collection and environmental education."
  },
  {
    "objectID": "research.html#specialist-herbivore-performance-on-introduced-plants-during-native-host-decline",
    "href": "research.html#specialist-herbivore-performance-on-introduced-plants-during-native-host-decline",
    "title": "Grace M. Horne",
    "section": "Specialist Herbivore Performance on Introduced Plants During Native Host Decline",
    "text": "Specialist Herbivore Performance on Introduced Plants During Native Host Decline\n\n\n\n\n\nMy senior undergraduate thesis was published in Environmental Entomology in January 2023. It is free to view at this link: https://academic.oup.com/ee/advance-article/doi/10.1093/ee/nvac107/6968949.\nDuring the socially-distant summer of 2020, I raised hundreds of caterpillars with the goal of helping to answer a single question: what will become of ash-eating insects once ash disappears (or nearly disappears) from New England and northeastern forests? To chip away at this mammoth question—all insects are sure to respond differently! (how differently is the true question)—I reared three different species of hawkmoth caterpillars on four different host plants in the same plant family as their primary host plant, ash.\nOver the course of the experiment, I started to notice differences in how the three species tolerated each host plant: Sphinx kalmiae did quite well on lilac, but Sphinx chersis perished entirely on the same plant. It’s these differences that became the theme of my narrative. The scope of the problem of disappearing ash will not be discovered by painting in broad strokes."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "**Grace M. Horne**",
    "section": "Education",
    "text": "Education\n2021 – present\nUniversity of California, Davis\nPh.D. Entomology, in progress\nAdvisor: Dr. Emily Meineke\n2017 – 2021\nColby College, Waterville, ME\nB.A. Biology (honors) & Environmental Science\nMagna cum laude with distinction in both majors\nThesis: “Reduced performance of ash-specialist caterpillars on non-native, cultivated Oleaceous plants”\nSpring 2020\nRound River Conservation Studies, Maun, Botswana\n\nRound River Conservation Studies (RRCS) is an environmental organization operating at the nexus of conservation and education. We explored the complex relationship between conservation, people, and wildlife, mostly in Mababe, Botswana. We worked with local experts to design and maintain wildlife monitoring systems to be used to substantiate economic and environmental decisions. This program helped me to develop fluency in field work, responses to human-wildlife conflict, and communication with local stakeholders."
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Grace M. Horne",
    "section": "",
    "text": "Links\nBelow is a working list of the resources I use for my research and life. You can view my current public projects on my GitHub profile, @gracehorne.\n\nR\n\nR Basics\nR for Data Science\nStack Overflow\n(R) Coding Club\nHappy Git with R\n\n\n\nUCD\nEntomology\n\nDiversity statement\nProspective students\n\nCampus-wide\n\nMental health support\nCenter for Advocacy, Resources, & Education (CARE)\n\n\n\n\n\nMedia & Press"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Grace M. Horne",
    "section": "",
    "text": "Tutorials\nI’m currently leading a data group at UC Davis for entomology and entomology-adjacent students. Occasionally, we invite guest speakers or follow tutorials to enhance our knowledge of data science in biology. Here, I’m working on compiling tutorials for our group and the general public.\n\nwRiting functions in R",
    "crumbs": [
      "⁴ Tutorials"
    ]
  },
  {
    "objectID": "iterativecode.html",
    "href": "iterativecode.html",
    "title": "Grace M. Horne",
    "section": "",
    "text": "Writing iterative code\n\nWhat is functional programming?\n\nDefinition here\n\n\nFor this tutorial, we will be working with the built-in data set co2. This is a simple data set of the monthly atmospheric CO2 concentrations at the Mauna Loa Observatory in Hawaii from 1959 to 1997.\nTo start, we need to load some packages and our data. As is, the data is in the form of a time series. We want to convert it to a data frame.\n\n# load necessary packages - make sure you have these installed!\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(data.table)\n\n# load Mauna Loa Atmospheric CO2 Concentration data\ndata(co2)\n\n# convert to a data frame\nco2 &lt;-\n  data.frame(\n    CO2 = as.numeric(co2),\n    month = rep(month.abb, 39),\n    year = rep(1959:1997, each = 12)\n  )\n\n# new column for date\nco2 &lt;- co2 %&gt;% # create a date column and convert to date format\n  # we know the measurement is taken on the 15th\n  mutate(date = paste(\"15\", month, year) %&gt;% \n           as.Date(., format = \"%d %b %Y\")) \n\nLet’s explore the data! Here are some easy ways to get a feel for your data.\n\nhead(co2) # view the first couple of rows of data\n\n     CO2 month year       date\n1 315.42   Jan 1959 1959-01-15\n2 316.31   Feb 1959 1959-02-15\n3 316.50   Mar 1959 1959-03-15\n4 317.56   Apr 1959 1959-04-15\n5 318.13   May 1959 1959-05-15\n6 318.00   Jun 1959 1959-06-15\n\ntail(co2) # view the last couple of rows of data\n\n       CO2 month year       date\n463 364.52   Jul 1997 1997-07-15\n464 362.57   Aug 1997 1997-08-15\n465 360.24   Sep 1997 1997-09-15\n466 360.83   Oct 1997 1997-10-15\n467 362.49   Nov 1997 1997-11-15\n468 364.34   Dec 1997 1997-12-15\n\nstr(co2) # take a look at the classes of data\n\n'data.frame':   468 obs. of  4 variables:\n $ CO2  : num  315 316 316 318 318 ...\n $ month: chr  \"Jan\" \"Feb\" \"Mar\" \"Apr\" ...\n $ year : int  1959 1959 1959 1959 1959 1959 1959 1959 1959 1959 ...\n $ date : Date, format: \"1959-01-15\" \"1959-02-15\" ...\n\n# make a line plot of the data\n# note that your plot will probably look aesteically different than\n# mine because I am using a base R inspired gg theme\nggplot(data = co2, aes(x = date, y = CO2)) + \n  geom_line(alpha = 0.5, color = \"red\")\n\n\n\n\n\n\n\n\nNow this is all well and good, but what if we wanted to know what happens after 1997. After all, some of the people in this room were born after that date and we want to know what has happened in our lifetimes.\n\n\n\n\n\n\nThis statement may cause some contention in the room — proceed with caution gen-Zers.\n\n\n\n\n\n\nOkay, let’s grab some data from the Scripps CO2 program. Because we are all hackers in the room, we are going to scrape data from the website programatically instead of downloading it and then importing it. We will write a function to do this for all of the sampling station locations we are interested in! Before we even think about writing a function, we want to make sure our code works for one site.\nNavigate to the following website and pick your favorite sampling station: https://scrippsco2.ucsd.edu/data/atmospheric_co2/sampling_stations.html\nFor this example, I am going to pick our old friend the Mauna Loa Observatory, but I want you to pick a different one.\n\n# URL of the website containing the .csv files\nurl &lt;- \"https://scrippsco2.ucsd.edu/data/atmospheric_co2/mlo.html\"\n\n# read the HTML content of the webpage\nwebpage &lt;- read_html(url)\n\n# find all links on the webpage\nlinks &lt;- webpage %&gt;% \n  html_nodes(\"a\") %&gt;% \n  html_attr(\"href\")\n\n# filter links that point to CSV files then\n# filter the links to only the file containing flask CO2 monthly readings\ncsv_links &lt;- tibble(link = links) %&gt;%\n  filter(str_detect(link, \".csv$\"),\n         str_detect(link, \"flask_co2/monthly\")) \n\nlink2file &lt;- csv_links[1]\n\n# download and import CSV file into R\nfile_url &lt;- paste0(\"https://scrippsco2.ucsd.edu\", link2file)\nfilename &lt;- basename(file_url)\ndownload.file(file_url, destfile = filename, mode = \"wb\")\ndata1 &lt;- read.csv(filename)\n\nview(data1)\n\nAck! There’s a bunch of junk at the top of the file. Let’s wrangle this sucker real fast. We know from viewing the file that the first column header is Yr so let’s just skip everything before that.\n\n# skip everything until the `Yr` column\ndata &lt;- fread(filename, \n      skip = \"Yr\", check.names = TRUE)\n\n# remove downloaded files\nfile.remove(filename)\n\nview(data)\n\nThat looks much better. All that junk was metadata. I’ll copy it below, but if you don’t trust me, you can go back and view data1.\n\nThe data file below contains 10 columns. Columns 1-4 give the dates in several redundant formats. Column 5 below gives monthly Mauna Loa CO2 concentrations in micro-mol CO2 per mole (ppm), reported on the 2012 SIO manometric mole fraction scale. This is the standard version of the data most often sought. The monthly values have been adjusted to 24:00 hours on the 15th of each month. Column 6 gives the same data after a seasonal adjustment to remove the quasi-regular seasonal cycle. The adjustment involves subtracting from the data a 4-harmonic fit with a linear gain factor. Column 7 is a smoothed version of the data generated from a stiff cubic spline function plus 4-harmonic functions with linear gain. Column 8 is the same smoothed version with the seasonal cycle removed. Column 9 is identical to Column 5 except that the missing values from Column 5 have been filled with values from Column 7. Column 10 is identical to Column 6 except missing values have been filled with values from Column 8. Missing values are denoted by -99.99. Column 11 is the 3-digit sampling station identifier.\n\nAlright. We know right off the bat that we are going to have to do some more wrangling here. I want to rename the columns to something that makes sense to me, then I want to delete the rows without data, then I want to replace missing values with NAs, and then I want to make the date column of class date.\n\n# make a vector of new names\nnew_names &lt;- c(\n  \"year\",\n  \"month\",\n  \"date_excel\",\n  \"date\",\n  \"co2\",\n  \"co2_season_adj\",\n  \"co2_smooth\",\n  \"co2_smooth_season_adj\",\n  \"co2_interpolated\",\n  \"co2_season_adj_interpolated\"\n)\n\n# replace old names\ndata &lt;- data %&gt;%\n  rename_with(~new_names, everything())\n\n# delete rows with no data\ndata &lt;- data %&gt;% slice(-c(1:2))\n\n# replace -99.99 or NaN with NAs\ndata &lt;- data %&gt;%\n  mutate_all(~ ifelse(.x == -99.99 | .x == \"NaN\", NA_real_, .x))\n\n# reformat date column\ndata &lt;- data %&gt;%\n  mutate(date = as.Date(as.numeric(date_excel) - 1, \n                        origin = \"1899-12-30\"))\n         \nview(data)\n\nYay, that looks nice :). I think we are ready to start making our functions!"
  },
  {
    "objectID": "iterativecode.html#footnotes",
    "href": "iterativecode.html#footnotes",
    "title": "Grace M. Horne",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nnote!↩︎"
  },
  {
    "objectID": "writing-functions-in-R.html",
    "href": "writing-functions-in-R.html",
    "title": "Grace M. Horne",
    "section": "",
    "text": "printer-friendly version\n  \n\n\n\nwRiting functions in R\nR is a functional programming language, meaning that it provides tools for writing and manipulating functions (Wickham et al. 2019). Functional programming contrasts with imperative programming where the main focus is on how an operation is performed. Imperative programming allows the programmer to follow exactly what is going on throughout the process. Both approaches allow us to write iterative code.\n\n\nPart I: how to repeat yourself\n\nLet’s run through an example of iterative code exemplifying each approach. Say we want to find the median petal length for each species of Iris in the built-in iris dataset. We could just could just copy and paste:\n\n# load the tidyverse\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# load iris data\ndata(iris)\n\n# median petal length for Iris setosa\nmedian(iris$Petal.Length[which(iris$Species == \"setosa\")])\n\n[1] 1.5\n\n# median petal length for Iris versicolor\nmedian(iris$Petal.Length[which(iris$Species == \"versicolor\")])\n\n[1] 4.35\n\n# median petal length for Iris virginica\nmedian(iris$Petal.Length[which(iris$Species == \"virginica\")])\n\n[1] 5.55\n\n\nThis method might cause headaches if we were to make a mistake in editing the code or if we have more than three medians to calculate. Let’s see if we can use a more scalable approach, the for loop. A for loop has 2 main components (Martin 2020):\n\nThe first line dictates how many times we want to loop\nThe bracketed code defines the repeated code\n\n\n# create a vector to store the output\npetal_medians &lt;- vector()\n\n# loop through the species\nfor (species in unique(iris$Species)) {\n  petal_medians[species] &lt;-\n    median(iris$Petal.Length[which(iris$Species == species)])\n}\n\nLet’s motivate the use of functions. We will write a function that does the same thing as above, but faster and with more customization. For example, we can perform the same function over and over on different data frames instead of copying and pasting the for loop for each data frame. Moreover, functions provide at least three major advantages over other strategies (Martin 2020):\n\nthey can have names which makes code easier to understand\nif your requirements change, you only have 1 place to edit\nthey minimize copy-paste errors\n\n\n\n\n\n\n\nIt’s typically recommended that you should write a function when you’ve copy and pasted code at least once (follow the DRY principle: “don’t repeat yourself”)\n\n\n\n\n\n\nYou can think about functions as building blocks for solving data science problems. Functions are “first class objects” and can be treated just like other R objects (Peng 2012). For instance, they can be passed as arguments to other functions and can be nested. Functions are created using the function() directive and have the form:\n\nfunctionName &lt;- function(x, y = 1) {\n  # function body here\n}\n\nLet’s put our iris example in function form.\n\n# create function \nspeciesMedian &lt;- function(df = df, col = \"Petal.Length\") {\n  medians &lt;- vector()\n  \n  for (species in unique(iris$Species)) {\n    medians[species] &lt;-\n      median(iris[iris$Species == species, col])\n  }\n  medians\n} \n\n# execute function \nspeciesMedian(iris)\n\n    setosa versicolor  virginica \n      1.50       4.35       5.55 \n\n\n\n\n\n\n\n\nWhen drafting a function, first identify the inputs (e.g., df and col from speciesMedian())\n\n\n\n\n\n\nIn the example above, speciesMedian() has two arguments df and col. col has a default value of \"Petal.Length\" meaning that if you don’t change its value when you call the function, it will automatically be set to \"Petal.Length\". R function arguments are matched either by position or by name. This means that writing speciesMedian(iris, \"Petal.Length\") (matched by position) and speciesMedian(col = \"Petal.Length\", df = iris) will do the same thing. You can test it out to prove it to yourself!\n\n\n\n\n\n\nUsually first arguments contain data (e.g., df) and the last ones contain calculation details (e.g., col)\n\n\n\n\n\n\nYou may have intuited that I added a little extra functionality into the function speciesMedian(). Because I added an argument for col, we are able to use this function to calculate the median value of other columns. Let’s see this in practice.\n\n# cols in iris\ncolnames(iris)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\n# median sepal length per species\nspeciesMedian(iris, \"Sepal.Length\")\n\n    setosa versicolor  virginica \n       5.0        5.9        6.5 \n\n# median sepal width per species\nspeciesMedian(iris, \"Sepal.Width\")\n\n    setosa versicolor  virginica \n       3.4        2.8        3.0 \n\n\n\n\n\n\n\n\nObjects within a function reset each time the function is called (e.g., medians within the speciesMedian() function does not exist in your global environment)\n\n\n\n\n\n\nWhen we made our function, you may have noticed that we combined the for loop and function approaches into a single composite. This method still suffers from the slowness and the clunkiness of for loops. To supercharge our median calculations, let’s use the purr package from the tidyverse.\nThe purrr package provides map() functions which take a function and a series of elements to apply the function on. Once we gather all of the building blocks for our data science problem, we can use purrr to combine them.\n\n# let's use the map function to calculate the median of each column\n# for each species \nmap(split(iris, iris$Species), # split the iris data by species\n        ~ summarize(.x  %&gt;% group_by(Species), # group by species\n            across(everything(), # iterate over every column\n                   median)) # calculate the median\n) %&gt;% list_rbind() # bind into a single df\n\n# A tibble: 3 × 5\n  Species    Sepal.Length Sepal.Width Petal.Length Petal.Width\n  &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 setosa              5           3.4         1.5          0.2\n2 versicolor          5.9         2.8         4.35         1.3\n3 virginica           6.5         3           5.55         2  \n\n# I was trying to illustrate my point using map functions\n# but there's actually a much simpler way to achieve the same\n# result using dplyr\n\nsummarize(iris %&gt;% group_by(Species), across(everything(), median))\n\n# A tibble: 3 × 5\n  Species    Sepal.Length Sepal.Width Petal.Length Petal.Width\n  &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 setosa              5           3.4         1.5          0.2\n2 versicolor          5.9         2.8         4.35         1.3\n3 virginica           6.5         3           5.55         2  \n\n\nThe names of the different map() functions are derived from your desired output (Wright et al. 2021). For example,\n\nmap() - returns a list\nmap_lgl() - returns a logical vector\nmap_int() - returns an integer vector\nmap_dbl() - returns a double vector\nmap_chr() - returns a character vector\n\n\n\n\nPart II: flexing your skills\n\nFor Part II of this tutorial, we will be working with the built-in data set co2. This is a simple data set of the monthly atmospheric CO2 concentrations at the Mauna Loa Observatory in Hawaii from 1959 to 1997.\nTo start, we need to load some packages and our data. As is, the data is in the form of a time series. We want to convert it to a data frame.\n\n# load necessary packages - make sure you have these installed!\nlibrary(rvest)\nlibrary(data.table)\n\n# load Mauna Loa Atmospheric CO2 Concentration data\ndata(co2)\n\n# convert to a data frame\nco2 &lt;-\n  data.frame(\n    CO2 = as.numeric(co2),\n    month = rep(month.abb, 39),\n    year = rep(1959:1997, each = 12)\n  )\n\n# new column for date\nco2 &lt;- co2 %&gt;% # create a date column and convert to date format\n  # we know the measurement is taken on the 15th\n  mutate(date = paste(\"15\", month, year) %&gt;% \n           as.Date(., format = \"%d %b %Y\")) \n\nLet’s explore the data! Here are some easy ways to get a feel for your data.\n\nhead(co2) # view the first couple of rows of data\n\n     CO2 month year       date\n1 315.42   Jan 1959 1959-01-15\n2 316.31   Feb 1959 1959-02-15\n3 316.50   Mar 1959 1959-03-15\n4 317.56   Apr 1959 1959-04-15\n5 318.13   May 1959 1959-05-15\n6 318.00   Jun 1959 1959-06-15\n\ntail(co2) # view the last couple of rows of data\n\n       CO2 month year       date\n463 364.52   Jul 1997 1997-07-15\n464 362.57   Aug 1997 1997-08-15\n465 360.24   Sep 1997 1997-09-15\n466 360.83   Oct 1997 1997-10-15\n467 362.49   Nov 1997 1997-11-15\n468 364.34   Dec 1997 1997-12-15\n\nstr(co2) # take a look at the classes of data\n\n'data.frame':   468 obs. of  4 variables:\n $ CO2  : num  315 316 316 318 318 ...\n $ month: chr  \"Jan\" \"Feb\" \"Mar\" \"Apr\" ...\n $ year : int  1959 1959 1959 1959 1959 1959 1959 1959 1959 1959 ...\n $ date : Date, format: \"1959-01-15\" \"1959-02-15\" ...\n\n# make a line plot of the data\n# note that your plot will probably look aesthetically different than\n# mine because I am using a base R inspired gg theme\nggplot(data = co2, aes(x = date, y = CO2)) + \n  geom_line(alpha = 0.5, color = \"red\")\n\n\n\n\n\n\n\n\nNow this is all well and good, but what if we wanted to know what happens after 1997. After all, some of the people in this room were born after that date and we want to know what has happened in our lifetimes.\nOkay, let’s grab some data from the Scripps CO2 program. Because we are all hackers in the room, we are going to scrape data from the website programatically instead of downloading it and then importing it. We will write a function to do this for all of the sampling station locations we are interested in! Before we even think about writing a function, we want to make sure our code works for one site.\nNavigate to the following website and pick your favorite sampling station which has monthly flask CO2 data: https://scrippsco2.ucsd.edu/data/atmospheric_co2/sampling_stations.html\nFor this example, I am going to pick our old friend the Mauna Loa Observatory, but I want you to pick a different one.\n\n# URL of the website containing the .csv files\nurl &lt;- \"https://scrippsco2.ucsd.edu/data/atmospheric_co2/mlo.html\"\n\n# read the HTML content of the webpage\nwebpage &lt;- read_html(url)\n\n# find all links on the webpage\nlinks &lt;- webpage %&gt;% \n  html_nodes(\"a\") %&gt;% \n  html_attr(\"href\")\n\n# filter links that point to CSV files then\n# filter the links to only the file containing flask CO2 monthly readings\ncsv_links &lt;- tibble(link = links) %&gt;%\n  filter(str_detect(link, \".csv$\"),\n         str_detect(link, \"flask_co2/monthly\")) \n\nlink2file &lt;- csv_links[1]\n\n# download and import CSV file into R\nfile_url &lt;- paste0(\"https://scrippsco2.ucsd.edu\", link2file)\nfilename &lt;- basename(file_url)\ndownload.file(file_url, destfile = filename, mode = \"wb\")\ndata1 &lt;- read.csv(filename)\n\nview(data1)\n\nAck! There’s a bunch of junk at the top of the file. Let’s wrangle this sucker real fast. We know from viewing the file that the first column header is Yr so let’s just skip everything before that.\n\n# skip everything until the `Yr` column\ndata &lt;- fread(filename, \n      skip = \"Yr\", check.names = TRUE)\n\n# remove downloaded files\nfile.remove(filename)\n\nview(data)\n\nThat looks much better. All that junk was metadata. I’ll copy it below, but if you don’t trust me, you can go back and view data1.\n\nThe data file below contains 10 columns. Columns 1-4 give the dates in several redundant formats. Column 5 below gives monthly Mauna Loa CO2 concentrations in micro-mol CO2 per mole (ppm), reported on the 2012 SIO manometric mole fraction scale. This is the standard version of the data most often sought. The monthly values have been adjusted to 24:00 hours on the 15th of each month. Column 6 gives the same data after a seasonal adjustment to remove the quasi-regular seasonal cycle. The adjustment involves subtracting from the data a 4-harmonic fit with a linear gain factor. Column 7 is a smoothed version of the data generated from a stiff cubic spline function plus 4-harmonic functions with linear gain. Column 8 is the same smoothed version with the seasonal cycle removed. Column 9 is identical to Column 5 except that the missing values from Column 5 have been filled with values from Column 7. Column 10 is identical to Column 6 except missing values have been filled with values from Column 8. Missing values are denoted by -99.99. Column 11 is the 3-digit sampling station identifier.\n\nAlright. We know right off the bat that we are going to have to do some more wrangling here. I want to rename the columns to something that makes sense to me, then I want to delete the rows without data, then I want to replace missing values with NAs, and then I want to convert the date column into class Date. We also want to make sure all of the measurement columns are in numeric form.\n\n# make a vector of new names\nnew_names &lt;- c(\n  \"year\",\n  \"month\",\n  \"date_excel\",\n  \"date\",\n  \"co2\",\n  \"co2_season_adj\",\n  \"co2_smooth\",\n  \"co2_smooth_season_adj\",\n  \"co2_interpolated\",\n  \"co2_season_adj_interpolated\"\n)\n\n# replace old names\ndata &lt;- data %&gt;%\n  rename_with(~new_names, everything())\n\n# delete rows with no data\ndata &lt;- data %&gt;% slice(-c(1:2))\n\n# replace -99.99 or NaN with NAs\ndata &lt;- data %&gt;%\n  mutate_all(~ ifelse(.x == -99.99 | .x == \"NaN\", NA_real_, .x))\n\n# reformat date column\ndata &lt;- data %&gt;%\n  mutate(date = as.Date(as.numeric(date_excel) - 1, \n                        origin = \"1899-12-30\"))\n\n# reformat measurement columns\ndata &lt;- data %&gt;%\n  mutate_at(vars(starts_with(\"co2\")), as.numeric)\n         \nview(data)\n\nYay, that looks nice :). Let’s explore this data set just like we did the built-in co2 data set.\n\nhead(data) # view the first couple of rows of data\n\n   year month date_excel       date    co2 co2_season_adj co2_smooth\n1: 1960     1      21930 1960-01-14     NA             NA     316.32\n2: 1960     2      21961 1960-02-14     NA             NA     316.97\n3: 1960     3      21990 1960-03-14 317.70         316.29     317.80\n4: 1960     4      22021 1960-04-14 319.08         316.57     318.97\n5: 1960     5      22051 1960-05-14     NA             NA     319.48\n6: 1960     6      22082 1960-06-14     NA             NA     318.72\n   co2_smooth_season_adj co2_interpolated co2_season_adj_interpolated\n1:                316.23           316.32                      316.23\n2:                316.31           316.97                      316.31\n3:                316.39           317.70                      316.29\n4:                316.47           319.08                      316.57\n5:                316.54           319.48                      316.54\n6:                316.62           318.72                      316.62\n\ntail(data) # view the last couple of rows of data\n\n   year month date_excel       date    co2 co2_season_adj co2_smooth\n1: 2023     7      45122 2023-07-14 421.06         420.81     421.42\n2: 2023     8      45153 2023-08-14 419.69         421.65     419.49\n3: 2023     9      45184 2023-09-14 418.20         421.70     418.23\n4: 2023    10      45214 2023-10-14     NA             NA     418.73\n5: 2023    11      45245 2023-11-14     NA             NA     420.43\n6: 2023    12      45275 2023-12-14     NA             NA     421.93\n   co2_smooth_season_adj co2_interpolated co2_season_adj_interpolated\n1:                421.17           421.06                      420.81\n2:                421.45           419.69                      421.65\n3:                421.73           418.20                      421.70\n4:                422.00           418.73                      422.00\n5:                422.28           420.43                      422.28\n6:                422.55           421.93                      422.55\n\nstr(data) # take a look at the classes of data\n\nClasses 'data.table' and 'data.frame':  768 obs. of  10 variables:\n $ year                       : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ month                      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ date_excel                 : chr  \"21930\" \"21961\" \"21990\" \"22021\" ...\n $ date                       : Date, format: \"1960-01-14\" \"1960-02-14\" ...\n $ co2                        : num  NA NA 318 319 NA ...\n $ co2_season_adj             : num  NA NA 316 317 NA ...\n $ co2_smooth                 : num  316 317 318 319 319 ...\n $ co2_smooth_season_adj      : num  316 316 316 316 317 ...\n $ co2_interpolated           : num  316 317 318 319 319 ...\n $ co2_season_adj_interpolated: num  316 316 316 317 317 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\n# make a line plot of the data\nggplot(data = data, aes(x = date, y = co2)) + \n  geom_line(alpha = 0.5, color = \"red\")\n\nWarning: Removed 5 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n# looks good! there are some missing values at the start\n\nI think we are ready to start making our functions! Let’s try to grab all of the sites with flask CO2 data and bind them all into one data frame. For simplicity, here’s a list of all of the sites which meet that requirement:\n\nAlert, NWT, Canada\nMauna Loa Observatory, Hawaii\nCape Kumukahi, Hawaii\nFanning Island\nChristmas Island\nAmerican Samoa\nKermadec Islands\nBaring Head, New Zealand\n\nLet’s make a series of functions for this problem: one to get the data urls, one to download and load the data into R, and one to wrangle it. Firstly, we want to get the URLs for each of the above sites.\n\n# function to find matches within a vector from a given URL\ngetURLs &lt;- function(url, vector) {\n  # read the HTML content from the given URL\n  html &lt;- read_html(url)\n  \n  # extract text from &lt;td class=\"rollover-links\"&gt; tags\n  matches &lt;- html_nodes(html, \"td.rollover-links\") %&gt;%\n    html_text()\n  \n  # find matches within the vector\n  matched_vector &lt;- vector[vector %in% matches]\n  \n  # E]extract href attributes corresponding to the matched text\n  href_matches &lt;- html_nodes(html, \"td.rollover-links\") %&gt;%\n    html_nodes(\"a\") %&gt;%\n    html_attr(\"href\")\n  \n  # prepend the base URL string to each href attribute\n  href_matches &lt;- paste0(\"https://scrippsco2.ucsd.edu/data/atmospheric_co2/\", href_matches)\n  \n  # return URLs only for the matches within the vector\n  return(href_matches[matches %in% matched_vector])\n  \n}\n\nurl &lt;- \"https://scrippsco2.ucsd.edu/data/atmospheric_co2/sampling_stations.html\"\nsites &lt;- c(\"Alert, NWT, Canada\", \"Mauna Loa Observatory, Hawaii\", \"Cape Kumukahi, Hawaii\", \"Fanning Island\", \"Christmas Island\", \"American Samoa\", \"Kermadec Island\", \"Baring Head, New Zealand\")\n\nmatched_urls &lt;- getURLs(url = url, vector = sites)\nprint(matched_urls)\n\n[1] \"https://scrippsco2.ucsd.edu/data/atmospheric_co2/alt.html\"\n[2] \"https://scrippsco2.ucsd.edu/data/atmospheric_co2/mlo.html\"\n[3] \"https://scrippsco2.ucsd.edu/data/atmospheric_co2/kum.html\"\n[4] \"https://scrippsco2.ucsd.edu/data/atmospheric_co2/fan.html\"\n[5] \"https://scrippsco2.ucsd.edu/data/atmospheric_co2/chr.html\"\n[6] \"https://scrippsco2.ucsd.edu/data/atmospheric_co2/sam.html\"\n[7] \"https://scrippsco2.ucsd.edu/data/atmospheric_co2/ker.html\"\n[8] \"https://scrippsco2.ucsd.edu/data/atmospheric_co2/nzd.html\"\n\n\nSweet! Now we have all of the URLs. Let’s download the data now. We need to edit our code from the earlier chunk where we supplied a URL, downloaded a .csv, and loaded it into R. Before we copied and pasted the site URL. Now we want to supply our function with our matched_urls vector and get the downloaded data.\n\n# function to scrape data from a URL\nscrapeData &lt;- function(url) {\n  # read the HTML content of the webpage\n  webpage &lt;- read_html(url)\n  \n  # find all links on the webpage\n  links &lt;- webpage %&gt;% \n    html_nodes(\"a\") %&gt;% \n    html_attr(\"href\")\n  \n  # filter links that point to CSV files then\n  # filter the links to only the file containing flask CO2 monthly readings\n  csv_links &lt;- tibble(link = links) %&gt;%\n    filter(str_detect(link, \".csv$\"),\n           str_detect(link, \"flask_co2/monthly\")) \n  \n  # if no CSV link is found, return NULL\n  if (nrow(csv_links) == 0) {\n    return(NULL)\n  }\n  \n  # select the first CSV link\n  link2file &lt;- csv_links$link[1]\n  file_url &lt;- paste0(\"https://scrippsco2.ucsd.edu\", link2file)\n  filename &lt;- basename(file_url)\n  \n  # skip everything until the `Yr` column and read the CSV\n  data &lt;- fread(file_url, skip = \"Yr\", check.names = TRUE)\n  \n  # return the data\n  return(data)\n}\n\n# scrape data from each URL and name each df after the site\ndfs &lt;- set_names(map(matched_urls, scrapeData), \n                 map(matched_urls, ~ sub(\"\\\\..*$\", \"\", basename(.x))))\n\n# filter out NULL values (if any)\ndfs &lt;- compact(dfs)\n\nOk, now that we have the data downloaded, let’s wrangle it and flatten it into a single data frame with a column identifying each site.\n\n# make a function to wrangle the scraped data\nprocessData &lt;- function(data, site_name) {\n  # make a vector of new names\n  new_names &lt;- c(\n    \"year\",\n    \"month\",\n    \"date_excel\",\n    \"date\",\n    \"co2\",\n    \"co2_season_adj\",\n    \"co2_smooth\",\n    \"co2_smooth_season_adj\",\n    \"co2_interpolated\",\n    \"co2_season_adj_interpolated\"\n  )\n  \n  # replace old names\n  data &lt;- data %&gt;%\n    rename_with(~ new_names, everything())\n  \n  # delete rows with no data\n  data &lt;- data %&gt;% slice(-c(1:2))\n  \n  # replace -99.99 or NaN with NAs\n  data &lt;- data %&gt;%\n    mutate_all(~ ifelse(.x == -99.99 | .x == \"NaN\", NA_real_, .x))\n  \n  # reformat date column\n  data &lt;- data %&gt;%\n    mutate(date = as.Date(as.numeric(date_excel) - 1, origin = \"1899-12-30\"))\n  \n  # reformat measurement columns and add a site column\n  data &lt;- data %&gt;%\n    mutate_at(vars(starts_with(\"co2\")), as.numeric) %&gt;%\n    mutate(site = site_name)\n  \n  return(data)\n}\n\n# process the data for each dataframe within the df list and\n# bind everything together\nco2_all &lt;- map2(dfs, names(dfs), processData) %&gt;% list_rbind()\n\nFinally, let’s look at our newly acquired data. We can use ggplot2 to create facets for each site and look at all of our data at once. We can also look at other columns such as co2_season_adj_interpolated.\n\n# make a line plot of the data and facet by site\nggplot(data = co2_all, aes(x = date, y = co2)) + \n  geom_line(alpha = 0.5, color = \"red\") + \n  facet_wrap(~ site, scales = \"free_x\")\n\nWarning: Removed 8 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n# make a line plot of the interpolated and seasonally adjusted \n# data and facet by site\nggplot(data = co2_all, \n       aes(x = date, y = co2_season_adj_interpolated)) + \n  geom_line(alpha = 0.5, color = \"red\") + \n  facet_wrap(~ site, scales = \"free_x\")\n\n\n\n\n\n\n\n\n\nGlossary\n\nfirst class objects\n\nobjects that are treated as any other data type\n\nfunctional programming\n\nthe computer is given a set of functions to be performed\n\nimperative programming\n\nthe computer is given a set of steps to accomplish the goal\n\niterative code\n\na set of instructions being repeated\n\npipe operator %&gt;%\n\ntakes the output of the expression on the left and passes it as the first argument of the function on the right\n\npurrr\n\na package providing tools for working with functions and vectors\n\ntidyverse\n\na collection of open source packages built with “tidy data” design principles\n\n\n\n\n\n\n\n\n\nReferences\n\nMartin, Charles. 2020. “Functions and Iteration.” https://numerilab.io//en/workshops/FunctionsIteration.\n\n\nPeng, Roger. 2012. “Writing Functions.” https://www.youtube.com/watch?v=KIqlKw2zqEQ.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWright, Carrie, Shannon Ellis, Stephanie Hicks, and Roger D Peng. 2021. Tidyverse Skills for Data Science in R."
  },
  {
    "objectID": "metabolomics-analysis-with-brms.html",
    "href": "metabolomics-analysis-with-brms.html",
    "title": "Grace M. Horne",
    "section": "",
    "text": "printer-friendly version\n  \n\n\n\nmetabolomics analysis with brms\n&lt;Grace write intro&gt;\n\n\nPart 0: getting familiar with the data\n\nLet’s make sure we are set up for success. Let’s load into memory the two packages we need (tidyverse and brms) and the data. Please download the data using the button below and ensure that you are pointed to the proper working directory (hint: setwd()) before you load it.\n\n\n  \n    \n  \n  \n\n    download data\n  \n\n\n\n# required packages\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(bayesplot)\n\n# data frame (assuming your data are in the folder called \"files\")\ndat &lt;- read_csv(\"files/meineke-primary-metabolism-GCTOFMS.csv\")\n\n\nWrangling into a readable format\nFirst, let’s wrangle the data into a machine- and human-readable format. Pretty much, we want to take the metadata from the first few rows and add it back in the long format. Then, we want to scale the peak height variable so it is\n\n# get dimensions of data\ndim(dat)\n\n[1] 711  55\n\n# look at the first couple of rows\n# the first 7 columns are blank until the column names\nhead(dat, n = 10L)\n\n# A tibble: 10 × 55\n   ...1         ...2   ...3  ...4  ...5  ...6  ...7  `file id` `220518bKCsa07_1`\n   &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;            \n 1 &lt;NA&gt;         &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  mx class  656732           \n 2 &lt;NA&gt;         &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  mx sample 220518bKCsa07_1  \n 3 &lt;NA&gt;         &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  label     10_005           \n 4 &lt;NA&gt;         &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  comment   10               \n 5 &lt;NA&gt;         &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  species   leaves           \n 6 &lt;NA&gt;         &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  organ     leaves           \n 7 &lt;NA&gt;         &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  treatment City - City      \n 8 BinBase name ret.i… quan… BB id mass… PubC… KEGG  InChI Key &lt;NA&gt;             \n 9 xylulose     553450 173   31632 85:1… 4392… C003… LQXVFWRQ… 1511             \n10 xylose       544100 103   1147… 85:8… 1351… C001… SRBFZHDQ… 7805             \n# ℹ 46 more variables: `220518bKCsa19_1` &lt;chr&gt;, `220518bKCsa37_1` &lt;chr&gt;,\n#   `220518bKCsa06_1` &lt;chr&gt;, `220518bKCsa20_1` &lt;chr&gt;, `220518bKCsa36_1` &lt;chr&gt;,\n#   `220518bKCsa12_2` &lt;chr&gt;, `220518bKCsa38_1` &lt;chr&gt;, `220518bKCsa24_1` &lt;chr&gt;,\n#   `220518bKCsa04_1` &lt;chr&gt;, `220518bKCsa32_1` &lt;chr&gt;, `220518bKCsa41_1` &lt;chr&gt;,\n#   `220518bKCsa40_1` &lt;chr&gt;, `220518bKCsa15_1` &lt;chr&gt;, `220518bKCsa05_1` &lt;chr&gt;,\n#   `220518bKCsa31_1` &lt;chr&gt;, `220518bKCsa22_1` &lt;chr&gt;, `220518bKCsa34_1` &lt;chr&gt;,\n#   `220518bKCsa18_1` &lt;chr&gt;, `220518bKCsa10_1` &lt;chr&gt;, …\n\n# make an id row so we can match samples later\n# we are adding NAs for the blank columns for now\nvalues &lt;- as.character(c(rep(NA, 7), \"id\", 1:47))\nid_row &lt;- matrix(values, nrow = 1) %&gt;% as.data.frame()\n\n# make the col names the same as the data\nnames(id_row) &lt;- colnames(dat)\n\n# add the id row to the data frame\ndat &lt;- dat %&gt;% add_row(.before = 1, !!!id_row)\n\n# get the metadata (from the first 8 columns) and reformat\nmeta_dat &lt;- dat[1:8, 8:55] # get data\nnames(meta_dat) &lt;- as.vector(meta_dat[1,]) # rename\nmeta_dat &lt;- meta_dat %&gt;%\n  slice(-1) %&gt;%\n  column_to_rownames(var = \"id\") %&gt;%\n  t() %&gt;% \n  as.data.frame() %&gt;% \n  rowid_to_column(var = \"id\") %&gt;%\n  mutate(treatment = str_remove(treatment, \"-.*$\"),\n         treatment = tolower(treatment),\n         id = as.character(id)) %&gt;%\n  rename_with(~ str_replace_all(., \" \", \"_\"), contains(\" \"))\n  \n# get rid of metadata from the full df\ndat &lt;- dat %&gt;% slice(-(2:8))\n\n# get the correct col names\nreplacement_ids &lt;- dat[1, ] %&gt;% as.character()\nreplacement_names &lt;- dat[2, ] %&gt;% as.character()\n\n# make a vector of new names\nnew_names &lt;- ifelse(str_detect(names(dat), \"\\\\.{3}|file id\"), replacement_names, replacement_ids)\n\n# rename columns\nnames(dat) &lt;- new_names\n\n# reformat data and rename columns\ndat &lt;- dat %&gt;% slice(-(1:2)) %&gt;%\n  pivot_longer(cols = \"1\":\"47\",\n               names_to = \"id\",\n               values_to = \"peak_height\") %&gt;%\n  rename_with( ~ str_replace_all(., \"[. ]\", \"_\"), \n               matches(\"[. ]\")) %&gt;%\n  rename(compound = BinBase_name, InChI = InChI_Key) %&gt;%\n  full_join(., meta_dat, by = \"id\", keep = FALSE) %&gt;%\n  select(mx_sample, everything(), peak_height, -id) \n\n# make sure data are of proper class\ndat &lt;- dat %&gt;%\n  mutate(peak_height = as.numeric(peak_height))\n\n# scale peak heights\ndat &lt;- dat %&gt;% \n  group_by(compound) %&gt;%\n  mutate(peak_heightZ = scale(peak_height)) %&gt;%\n  ungroup()\n\n# remove extra objects\nall_objects &lt;- ls() # list all objects\n\n# remove all objects except the df\nrm(list = setdiff(all_objects, \"dat\"))\nrm(all_objects)\n\n\n\nInitial data exploration\nHere, we will pick a random compound (or compounds) and look at the distribution of the data. This will give us an opportunity to familiarize ourselves with the shape and values in our dataset. Remember, we scaled the data, so the histograms we are going to generate are deviations from the mean rather than peak heights.\n\n# set seed\nset.seed(7348923)\n\n# pick a random compound\npick &lt;- sample(unique(dat$compound), 1)\n\n# make a histogram of the random compound\ndat %&gt;%\n  filter(compound == pick) %&gt;%\n  ggplot(aes(x = peak_heightZ)) + \n  geom_histogram(bins = 10)\n\n\n\n\n\n\n\n\n\n\n\n\nPart 1: pick an initial model\n\nPick 3 compounds to model the “abundance” (our scaled peak heights) by treatment. You can inspect the dataset using either View(dat) or unique(dat$compound). We also want to group by compound. This makes the model hierarchical, nesting the effects of treatment within each compound.\nWhen you are actually modelling the data, you will use the entire dataset, but for our exploration we are working with a subset so that the model runs within the bounds of a class period.\nHint: the syntax for the formula is: response ~ pterms + (1 | group)\n\n# filter our dataset down to the 3 compounds and remove the \"pool\" treatment\ndat2 &lt;- dat %&gt;%\n  filter(treatment != \"pool\", compound == \"saccharic acid\" |\n           compound == \"kaempferol\" | compound == \"vanillic acid\")\n\n# write the model and store it as an object\ndefault_prior_mbrms &lt;- brms::brm(\n  peak_heightZ ~ treatment + (1 | compound),\n  data = dat2,\n  family = gaussian(),\n  chains = 3,\n  iter = 3000,\n  warmup = 1000,\n  cores = 3\n)\n\n\n\n\nPart 2: prior predictive check\n\n\n\n# use default_prior() on the model object to look at the values\n# brms automatically gives to the model\ndefault_prior(default_prior_mbrms)\n\n                   prior     class                   coef    group resp dpar\n                  (flat)         b                                          \n                  (flat)         b treatmenthighpollution                   \n                  (flat)         b       treatmenthighway                   \n                  (flat)         b  treatmentlowpollution                   \n                  (flat)         b          treatmentpool                   \n                  (flat)         b         treatmentrural                   \n student_t(3, -0.2, 2.5) Intercept                                          \n    student_t(3, 0, 2.5)        sd                                          \n    student_t(3, 0, 2.5)        sd                        compound          \n    student_t(3, 0, 2.5)        sd              Intercept compound          \n    student_t(3, 0, 2.5)     sigma                                          \n nlpar lb ub       source\n                  default\n             (vectorized)\n             (vectorized)\n             (vectorized)\n             (vectorized)\n             (vectorized)\n                  default\n        0         default\n        0    (vectorized)\n        0    (vectorized)\n        0         default\n\n# rerun the model with the arguement: sample_prior = \"only\"\n\ndefault_prior_check &lt;- brms::brm(\n  peak_heightZ ~ treatment + (1 | compound),\n  data = dat2,\n  family = gaussian(),\n  chains = 3,\n  iter = 3000,\n  warmup = 1000,\n  cores = 3,\n  sample_prior = \"only\"\n)\n\nError: Sampling from priors is not possible as some parameters have no proper priors. Error occurred for parameter 'b'.\n\n\nDid you get an error? That’s because the betas (b) have totally flat priors. You can think of that as the uniform distribution below where all outcomes are equally likely (improper prior).\n\n# create a data frame with x values for plotting\nx_values &lt;- seq(from = 0, to = 4, length.out = 100)\ndata &lt;- data.frame(x = x_values)\n\n# create the plot\nggplot(data, aes(x)) +\n  stat_function(fun = dunif, args = list(min = 0, max = 4)) +\n  ylim(0, 1)\n\n\n\n\n\n\n\n\n\n\n\nPart 3: modify the model\n\n\nBecause models with horseshoe priors are more likely than other models to have divergent transitions, we need to increase adapt_delta which will slow the sampler but should decrease the number of divergent transitions.\n\n# write the modified model and sample the prior *only*\nhorseshoe_prior_check &lt;- brm(\n  data = dat2,\n  family = gaussian,\n  peak_heightZ ~ treatment + (1 + treatment | compound),\n  prior = c(\n    prior(normal(0, 1), class = Intercept),\n    prior(\"horseshoe(3)\", class = b), \n    prior(exponential(1), class = sd),\n    prior(exponential(1), class = sigma)\n  ),\n  iter = 2000,\n  warmup = 1000,\n  chains = 3,\n  cores = 3,\n  sample_prior = \"only\"\n)\n\n# prior predictive check\npp_check(horseshoe_prior_check)\n\n\n\n\nPart 4: fit the model\n\n\n# run the modified model\nhorseshoe_prior_mbrms &lt;- brm(\n  data = dat2,\n  family = gaussian,\n  peak_heightZ ~ treatment + (1 + treatment | compound),\n  prior = c(\n    prior(normal(0, 1), class = Intercept),\n    prior(\"horseshoe(3)\", class = b), \n    prior(exponential(1), class = sd),\n    prior(exponential(1), class = sigma)\n  ),\n  iter = 2000,\n  warmup = 1000,\n  chains = 3,\n  cores = 3\n)\n\nCompiling Stan program...\n\n\nTrying to compile a simple C file\n\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 15.0.0 (clang-1500.3.9.4)’\nusing SDK: ‘MacOSX14.4.sdk’\nclang -arch arm64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Users/gracehorne/Library/R/arm64/4.4/library/Rcpp/include/\"  -I\"/Users/gracehorne/Library/R/arm64/4.4/library/RcppEigen/include/\"  -I\"/Users/gracehorne/Library/R/arm64/4.4/library/RcppEigen/include/unsupported\"  -I\"/Users/gracehorne/Library/R/arm64/4.4/library/BH/include\" -I\"/Users/gracehorne/Library/R/arm64/4.4/library/StanHeaders/include/src/\"  -I\"/Users/gracehorne/Library/R/arm64/4.4/library/StanHeaders/include/\"  -I\"/Users/gracehorne/Library/R/arm64/4.4/library/RcppParallel/include/\"  -I\"/Users/gracehorne/Library/R/arm64/4.4/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Users/gracehorne/Library/R/arm64/4.4/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from &lt;built-in&gt;:1:\nIn file included from /Users/gracehorne/Library/R/arm64/4.4/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Users/gracehorne/Library/R/arm64/4.4/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Users/gracehorne/Library/R/arm64/4.4/library/RcppEigen/include/Eigen/Core:19:\n/Users/gracehorne/Library/R/arm64/4.4/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n#include &lt;cmath&gt;\n         ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n\n\nStart sampling\n\n\nWarning: There were 3 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\n\n\nWarning: Examine the pairs() plot to diagnose sampling problems\n\n# get the stats\ncoef(horseshoe_prior_mbrms)\n\n$compound\n, , Intercept\n\n                   Estimate Est.Error       Q2.5     Q97.5\nkaempferol      0.004588458 0.1426425 -0.2726568 0.3025087\nsaccharic acid  0.020755326 0.1487281 -0.2607016 0.3226474\nvanillic acid  -0.036796999 0.1573251 -0.3610275 0.2698693\n\n, , treatmenthighpollution\n\n                  Estimate Est.Error       Q2.5     Q97.5\nkaempferol      0.01270065 0.2119322 -0.4272259 0.4580600\nsaccharic acid  0.14355073 0.2466807 -0.2730471 0.7371949\nvanillic acid  -0.02074919 0.2378982 -0.5350825 0.4656884\n\n, , treatmenthighway\n\n                   Estimate Est.Error       Q2.5     Q97.5\nkaempferol     -0.007478984 0.2274892 -0.4755780 0.4798898\nsaccharic acid  0.093680147 0.2490233 -0.3797785 0.6587649\nvanillic acid  -0.229665669 0.2801556 -0.8648843 0.2104319\n\n, , treatmentlowpollution\n\n                  Estimate Est.Error       Q2.5     Q97.5\nkaempferol     -0.01354197 0.2069538 -0.4514980 0.4107297\nsaccharic acid -0.10721044 0.2376890 -0.6715355 0.3125030\nvanillic acid  -0.03656352 0.2239383 -0.5262962 0.4185214\n\n, , treatmentpool\n\n                 Estimate Est.Error        Q2.5     Q97.5\nkaempferol      0.1510733 0.4282079 -0.69893393 1.0259892\nsaccharic acid -0.5827861 0.4661391 -1.50507353 0.3009077\nvanillic acid   1.2633713 0.5643529  0.09796665 2.3351627\n\n, , treatmentrural\n\n                  Estimate Est.Error       Q2.5     Q97.5\nkaempferol     -0.09676045 0.2203226 -0.6211980 0.3010464\nsaccharic acid -0.07468879 0.2253237 -0.5905028 0.3502682\nvanillic acid   0.01131759 0.2357427 -0.4721188 0.5151764\n\n\n\n\n\nPart 5: validate computation\n\n\npp_check(horseshoe_prior_mbrms)\n\nUsing 10 posterior draws for ppc type 'dens_overlay' by default.\n\n\n\n\n\n\n\n\npp_check(horseshoe_prior_mbrms, plotfun = \"boxplot\", nreps = 10, notch = FALSE)\n\nUsing 10 posterior draws for ppc type 'dens_overlay' by default.\n\n\nWarning: The following arguments were unrecognized and ignored: plotfun, nreps,\nnotch\n\n\n\n\n\n\n\n\n\n\n\n\nPart 6: addressing computation issues\n\n&lt;This part will be text only based on Gelman et al. 2020&gt;\n\n\n\nPart 7: evaluate the model\n\n\n\nPosterior predictive check\n\n\nInfluence of the prior\n\n\n\nPart 8: model is provisionally accepted (!!!)\n\n\n\nPlotting the model\n\n\nGlossary\n\n&lt;term&gt;\n\ndef"
  }
]